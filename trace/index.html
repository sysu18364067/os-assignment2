<head>
  <title>Peeking Blackjack</title>
  <script src="plugins/main.js"></script>
  <script src="grader-all.js"></script>
</head>

<body onload="onLoad('blackjack', '<a href=mailto:dilip@cs.stanford.edu>Dilip Arumugam<a>', 2)">

<div id="assignmentHeader"></div>

<p>
<div class="problemTitle">Submission Instructions</div>
<ul>
    <li> Deadline is Tuesday at <b>11pm PT</b>.

    <li> Submit on Gradescope and tag the correct pages. <b>Incorrectly tagged submissions will not receive credit.</b>
    <li> You can receive 1 extra credit point if you <b>type your whole assignment</b> and tag a page for the Typeset Bonus.
        If you did not type your whole assignment, please do not select a page for that rubric item. Assignments that
        are mostly written but have a typed page tagged also will not receive the bonus point.
    <li> Submit early and often. You can submit as many times as you'd like until the deadline:
        we will only grade the last submission.
    <li> For the autograder, submit early enough to check that the autograder output is correct. For example,
        if you use an outside library like numpy, the autograder will crash and <b>you will receive 0 points</b>.
        You are responsible for checking your program runs properly on the non-hidden test cases and does not
        crash overall.
    <li> We will not be accepting any assignments submitted via email.
</ul>
</p>

<p>
<img class="float-right" src="blackjack.jpg" style="width:260px;margin-left:10px"/>
</p>

<p>
The search algorithms explored in the previous assignment work great when you
know exactly the results of your actions.  Unfortunately, the real world is
not so predictable.  One of the key aspects of an effective AI is the ability
to reason in the face of uncertainty.
</p>

<p>
Markov decision processes (MDPs) can be used to formalize uncertain situations.
In this homework, you will implement algorithms to find the optimal policy in these situations.
You will then formalize a modified version of Blackjack as an MDP, and apply your algorithm
to find the optimal policy.
</p>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 1: Value Iteration</div>

<p>
In this problem, you will perform the value iteration updates manually on a
very basic game just to solidify your intuitions about solving MDPs.
The set of possible states in this game is $\mathcal{S} = \{-2, -1, 0, +1, +2\}$ and the set of possible actions is $\mathcal{A} = \{a_1, a_2\}$.  The initial state is $0$ and there are two terminal states, $-2$ and $+2$. Recall that the transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ encodes the probability of transitioning to a next state $s'$ after being in state $s$ and taking action $a$ as $\mathcal{T}(s'|s,a)$. In this MDP, the transition dynamics are given as follows:
<p>
$\forall i \in \{-1, 0, 1\} \subset \mathcal{S}$,
<ul>
         <li> $\mathcal{T}(i-1 | i, a_1) = 0.8$ and $\mathcal{T}(i+1 | i, a_1) = 0.2$
         <li> $\mathcal{T}(i-1 | i, a_2) = 0.7$ and $\mathcal{T}(i+1 | i, a_2) = 0.3$
</ul>
Think of this MDP as a chain formed by states $\{-2, -1, 0, +1, +2\}$. In words, action $a_1$ has a 80% chance of moving the agent backwards in the chain and a 20% chance of moving the agent forward. Similarly, action $a_2$ has a 70% of sending the agent backwards and a 30% chance of moving the agent forward. We will use a discount factor $\gamma = 1$. <br>
The reward function for this MDP is $\mathcal{R}(s,a,s') = \begin{cases} 20 & s' = -2 \\ 100 & s' = +2 \\ -5 & \text{otherwise} \end{cases}$

</p>

<ol class="problem">

<li class="writeup" id="1a">
Give the value of $V^\star_i(s)$ for each state in $\mathcal{S}$ after iterations $i = \{0, 1, 2\}$ of Value Iteration. Recall that $\forall s \in \mathcal{S}$, $V^\star_0(s) = 0$ and, for any terminal state $s_\text{terminal}$, $V^\star(s_\text{terminal}) = 0$. In words, all values are initialized to $0$ at iteration 0 and terminate states (for which the optimal policy is not defined) always have a value of $0$.

<li class="writeup" id="1b">
Using $V^\star_2(\cdot)$, what is the corresponding optimal policy $\pi^\star$ for all non-terminal states?

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: General MDP Results</div>

<p>
Equipped with an understanding of a basic algorithm for computing optimal value
functions in MDPs, let's deepen our understanding of MDPs and prove a few general results.
</br></br>

<b>In the parts that follow, the word "prove" means that we are expecting a formal, mathematical proof.</b>
</p>
</p>

<ol class="problem">

<li class="writeup" id="2a">
We begin with a MDP $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{T}, \gamma \rangle$. To simplify things a bit, we will take the reward function to be a function on state-action pairs, rather than transitions ($\mathcal{R}:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$). Consequently, we can write the Bellman equation as $V^\pi(s) = \mathcal{R}(s, \pi(s)) + \gamma \sum\limits_{s' \in \mathcal{S}} \mathcal{T}(s' | s,\pi(s)) V^\pi(s')$, for any policy $\pi:\mathcal{S} \rightarrow \mathcal{A}$. Let's also assume that $\mathcal{M}$ has no terminal states (the agent keeps selecting actions forever!).
<p>
Suppose we have an upper bound on the reward received at any timestep, $\max\limits_{s,a} \mathcal{R}(s,a) = R_\text{MAX}$. Prove that for any state $s \in \mathcal{S}$, $V^\pi(s) \leq \frac{R_\text{MAX}}{(1-\gamma)}$.

<p style="font-style: italic;">
Hint: Recall that $V^\pi(s)$ is the expected utility obtained by following policy $\pi(s)$ starting from state $s$, where the utility is the discounted sum of rewards: $u = r_1 + \gamma r_2 + \gamma^2 r_3 + \ldots$ and so on. Remember that $\mathcal{M}$ never terminates, so this will be an infinite sum. Once you apply the upper bound on rewards, what pattern do you see? What do you know about this kind of mathematical expression that allows you to eliminate the sum over all timesteps? Remember that $\gamma \in [0,1)$.
</p>
</p>

</li>

<li class="writeup" id="2b">
Keeping the setup from the previous part, now assume that you also have a lower bound of 0 on rewards: $0 \leq \mathcal{R}(s,a) \leq R_\text{MAX}, \forall s \in \mathcal{S},a \in \mathcal{A}$. Provide a simple modified reward function $\hat{\mathcal{R}}(s,a) = f(\mathcal{R}(s,a))$ and prove that the corresponding MDP is guaranteed to have $0 \leq V^\pi(s) \leq 1$, for any policy $\pi$. Finally, briefly explain (you should need at most two sentences) the relationship between the optimal policy of this new MDP and that of $\mathcal{M}$. Are they the same? Why or why not?

<p style="font-style: italic;">
Hint: For the second question, start by thinking about an MDP that always terminates after the agent takes exactly one step. Then, convince yourself that this answer holds for an agent acting for arbitrarily many steps.
</p>
</p>

</li>

<li class="writeup" id="2c">
In the previous part, you made a modification to the reward function and assessed its effect on the optimal policy of the original MDP. Let's do this again for a different kind of reward function manipulation and actually prove that it preserves the optimal policy. Just as in the first problem, it will be easier for us to work with a reward function operating on transitions, $\mathcal{R}:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$. 

<p>We will start with an initial MDP $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{T}, \gamma \rangle$ but would like to actually solve an MDP with an augmented reward function $\mathcal{M'} = \langle \mathcal{S}, \mathcal{A}, \mathcal{R'}, \mathcal{T}, \gamma \rangle$ where $\mathcal{R'}(s,a,s') = \mathcal{R}(s,a,s') + \mathcal{F}(s,a,s')$. Think of a scenario where $\mathcal{R}$ produces values of 0 for most transitions; a bonus reward function $\mathcal{F}:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ that produces non-zero values could provide us more immediate feedback and help accelerate the learning speed of our agent. In this problem, we will focus on a particular type of reward bonus $\mathcal{F}(s,a,s') = \gamma\phi(s') - \phi(s)$, for some arbitrary function $\phi:\mathcal{S} \rightarrow \mathbb{R}$. 

<p>First prove that $Q^\star_\mathcal{M}(s,a) - \phi(s) = Q^\star_\mathcal{M'}(s,a)$ and then use this fact to conclude that $\pi^\star_\mathcal{M'}(s) = \pi^\star_\mathcal{M}(s), \forall s \in \mathcal{S}$.

<p style="font-style: italic;">
    Hint: Start by using $Q^\star_\mathcal{M}(s,a)$, the Bellman optimality equation for $\mathcal{M}$, to expand the LHS (left hand side) of the first claim. Notice that $a - b = a + c - c - b$ for arbitrary values $a, b, c$. Make an $a-b$ expression in what you have. What value $c$ could you insert that allows you to incorporate $\mathcal{F}(s,a,s')$? Relate the resulting Bellman equation back to $\mathcal{M'}$. Finish by recalling how to express the optimal policy $\pi^\star_\mathcal{M'}(s)$ in terms of the optimal action-value function $Q^\star_\mathcal{M'}(s,a)$.
</p>
</p>
</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: Peeking Blackjack</div>

<p>
Now that we have gotten a bit of practice with general-purpose MDP algorithms, let's use them to play (a modified version of) Blackjack.
For this problem, you will be creating an MDP to describe states, actions, and rewards in this game. More specifically, after reading through the description of the
state representation and actions of our Blackjack game below, you will implement the transition and reward function of the Blackjack MDP inside <code>succAndProbReward()</code>.
</p>

<p>
For our version of Blackjack, the deck can contain an
arbitrary collection of cards with different face values.  At the start of the game,
    the deck contains the same number of each cards of each face value; we call this number
    the 'multiplicity'.  For example, a standard deck of 52 cards would have face values $[1, 2, \ldots,
13]$ and multiplicity 4.  You could also have a deck with face values
$[1,5,20]$; if we used multiplicity 10 in this case, there would be 30 cards in total (10 each of 1s, 5s, and 20s).
The deck is shuffled, meaning that each permutation of the cards is equally likely.
</p>

<p>
<img class="float-right" src="blackjack_rule.png" style="width:550px;margin-left:10px"/>
</p>

<p>
The game occurs in a sequence of rounds.
In each round, the player has three actions available to her:
<ul>
  <li> $a_\text{take}$ - Take the next card from the top of the deck.
  <li> $a_\text{peek}$ - Peek at the next card on the top of the deck.
  <li> $a_\text{stop}$ - Stop taking any more cards.
</ul>

</p>

<p>
In this problem, your state $s$ will be represented as a 3-element tuple:
<blockquote>
  <code>(totalCardValueInHand, nextCardIndexIfPeeked, deckCardCounts)</code>
</blockquote>
As an example, assume the deck has card values $[1, 2, 3]$ with multiplicity 1,
and the threshold is 4.
Initially, the player has no cards, so her total is 0;
this corresponds to state <code>(0, None, (1, 1, 1))</code>.
<ul>
  <li>For $a_\text{take}$, the three possible successor states (each with equal probability of $1/3$) are:
<blockquote>
  <code>(1, None, (0, 1, 1))</code><br>
  <code>(2, None, (1, 0, 1))</code><br>
  <code>(3, None, (1, 1, 0))</code><br>
</blockquote>
In words, a random card that is available in the deck is drawn and its corresponding count in the deck is then decremented. Remember that <code>succAndProbReward()</code> will expect you return all three of the successor states shown above. Note that $\mathcal{R}(s, a_\text{take}, s') = 0, \forall s,s' \in \mathcal{S}$. Even though the agent now has a card in her hand for which she may receive a reward at the end of the game, the reward is not actually granted until the game ends (see termination conditions below).
</p>

<p>
<li>For $a_\text{peek}$, the three possible successor states are:
<blockquote>
  <code>(0, 0, (1, 1, 1))</code><br>
  <code>(0, 1, (1, 1, 1))</code><br>
  <code>(0, 2, (1, 1, 1))</code><br>
</blockquote>
  Note that it is not possible to peek twice in a row; if the player peeks twice in a row, then <code>succAndProbReward()</code> should return <code>[]</code>. Additionally, $\mathcal{R}(s, a_\text{peek}, s') = -\text{peekCost}, \forall s,s' \in \mathcal{S}$. That is, the agent will receive an immediate reward of <code>-peekCost</code> for reaching any of these states. <br>
    Things to remember about the states after taking $a_\text{peek}$:
    <ul>
        <li>From <code>(0, 0, (1, 1, 1))</code>, taking a card will lead to the state <code>(1, None, (0, 1, 1))</code> deterministically (that is, with probability 1.0).</li>
        <li>
            The second element of the state tuple is not the face value of the card that will be drawn next, but the
            index into the deck (the third element of the state tuple) of the card that will be drawn next.  In other words,
            the second element will always be between 0 and <code>len(deckCardCounts)-1</code>, inclusive.
        </li>
    </ul>
</li>
    <li>
        For $a_\text{stop}$, the resulting state will be <code>(0, None, None)</code>.
        (Remember that setting the deck to <code>None</code> signifies the end of the game.)
    </li>
</ul>
</p>

<p>
The game continues until one of the following termination conditions becomes true:
<ul>
   <li>The player chooses $a_\text{stop}$, in which case her reward is the sum of the face values of the cards in her hand.
   <li>The player chooses $a_\text{take}$ and "goes bust".  This means that the sum of the face values
    of the cards in her hand is strictly greater than the threshold specified at the start
    of the game.  If this happens, her reward is 0.
   <li>The deck runs out of cards, in which case it is as if she selects $a_\text{stop}$, and she
gets a reward which is the sum of the cards in her hand. <span style="font-style: italic;">
    Make sure that if you take the last card and go bust, then the reward becomes 0 not
    the sum of values of cards.
    </span>
</ul>
</p>

As another example with our deck of $[1,2,3]$ and multiplicity 1, let's say the player's current state is <code>(3, None, (1, 1, 0))</code>, and the threshold remains 4.
<ul>
  <li>For $a_\text{stop}$, the successor state will be <code>(3, None, None)</code>.</li>
  <li>For $a_\text{take}$, the successor states are <code>(3 + 1, None, (0, 1, 0))</code> or <code>(3 + 2, None, None)</code>.
Note that in the second successor state, the deck is set to <code>None</code> to signify the game ended with a bust.
You should also set the deck to <code>None</code> if the deck runs out of cards.</li>
</ul>

<ol class="problem">

<li class="code" id="3a">
Implement the game of Blackjack as an MDP by filling out the
<code>succAndProbReward()</code> function of class <code>BlackjackMDP</code>.
</li>

<li class="code" id="3b">
Let's say you're running a casino, and you're trying to design a deck to make
people peek a lot.  Assuming a fixed threshold of 20, and a peek cost of 1,
design a deck where for at least 10% of states, the optimal policy is to peek.
Fill out the function <code>peekingMDP()</code> to return an instance of
<code>BlackjackMDP</code> where the optimal action is to peek in at least
10% of states. <span style="font-style: italic;">Hint: Before randomly
assinging values, think of the case when you really want to peek instead
of blindly taking a card. </span>
</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 4: Learning to Play Blackjack</div>

<p>
So far, we've seen how MDP algorithms can take an MDP which describes the full
dynamics of the game and return an optimal policy.  But suppose you go into
a casino, and no one tells you the rewards or the transitions.
We will see how reinforcement learning can allow you to play the game
and learn its rules & strategy at the same time!
</p>

<ol class="problem">

<li class="code" id="4a">
You will first implement a generic Q-learning algorithm <code>QLearningAlgorithm</code>,
which is an instance of an <code>RLAlgorithm</code>.  As discussed in class,
reinforcement learning algorithms are capable of executing a policy while
simultaneously improving that policy.  Look in <code>simulate()</code>, in
<code>util.py</code> to see how the <code>RLAlgorithm</code> will be used.  In
short, your <code>QLearningAlgorithm</code> will be run in a simulation of the MDP, and will
alternately be asked for an action to perform in a given state (<code>QLearningAlgorithm.getAction</code>), and then be
informed of the result of that action (<code>QLearningAlgorithm.incorporateFeedback</code>),
so that it may learn better actions to perform in the future.
</p>

<p>
We are using Q-learning with function approximation,
which means $\hat{Q}^\star(s, a) = \mathbb w \cdot \phi(s, a)$,
where in code, $\mathbb w$ is <code>self.weights</code>, $\phi$ is the <code>featureExtractor</code> function,
and $\hat{Q}^\star$ is <code>self.getQ</code>.
<p>
We have implemented <code>QLearningAlgorithm.getAction</code> as a simple $\epsilon$-greedy policy.
Your job is to implement <code>QLearningAlgorithm.incorporateFeedback()</code>,
which should take an $(s, a, r, s')$ tuple and update <code>self.weights</code>
according to the standard Q-learning update.
</p>
</li>

<li class="writeup" id="4b">
Now let's apply Q-learning to an MDP and see how well it performs
    in comparison with value iteration.  First, call <code>simulate</code>
    using your Q-learning code and the <code>identityFeatureExtractor()</code> on the MDP <code>smallMDP</code>
    (defined for you in <code>submission.py</code>), with 30000 trials.
    How does the Q-learning policy compare with a policy learned by value iteration
    (i.e., for how many states do they produce a different action)?
    (Don't forget to set the explorationProb of your Q-learning algorithm to 0 after learning the policy.)
    Now run <code>simulate()</code> on <code>largeMDP</code>, again with 30000 trials.  How does the policy
    learned in this case compare to the policy learned by value iteration?  What went wrong?


</li>

<li class="code" id="4c">
To address the problems explored in the previous exercise, let's incorporate some
domain knowledge to improve generalization.  This way, the algorithm can use
what it has learned about some states to improve its prediction performance on
other states. Implement <code>blackjackFeatureExtractor</code> as described in the code comments.
Using this feature extractor, you should be able to get pretty close to the
optimum on the <code>largeMDP</code>.
</li>

<li class="writeup" id="4d">
Sometimes, we might reasonably wonder how an optimal policy learned for one MDP
    might perform if applied to another MDP with similar structure but slightly
    different characteristics.  For example, imagine that you created an MDP to
    choose an optimal strategy for playing "traditional" blackjack, with a standard
    card deck and a threshold of 21.  You're living it up in Vegas
    every weekend, but the casinos get wise to your approach and decide to make
    a change to the game to disrupt your strategy: going forward, the threshold
    for the blackjack tables is 17 instead of 21.  If you continued playing the
    modified game with your original policy, how well would you do?  (This is just
    a hypothetical example; we won't look specifically at the blackjack game in
    this problem.)

    <p>
        To explore this scenario, let's take a brief look at how a policy learned
        using value iteration responds to a change in the rules of the MDP. For all subsequent parts, make sure to use 30,000 trials.
    </p>
    <ul>
        <li>
            First, run value iteration on the <code>originalMDP</code>
            (defined for you in <code>submission.py</code>) to compute an
            optimal policy for that MDP.
        </li>
        <li>
            Next, simulate your policy on <code>newThresholdMDP</code> (also defined for you in
            <code>submission.py</code>) by calling <code>simulate</code> with an instance of
            <code>FixedRLAlgorithm</code> that has been instantiated using the policy you computed
            with value iteration.  What is the expected reward from this simulation?
            <span style="font-style: italic;">Hint: read the documentation (comments) for the
            <code>simulate</code> function in util.py, and look specifically at the format of the
            function's return value.</span>
        </li>
        <li>
            Now try simulating Q-learning directly on <code>newThresholdMDP</code> with <code>blackjackFeatureExtractor</code> and the default exploration probability.
            What is your expected reward under the new Q-learning policy?  Provide some
            explanation for how the rewards compare, and why they are different.
        </li>
    </ul>


</li>

</ol>

</body>
